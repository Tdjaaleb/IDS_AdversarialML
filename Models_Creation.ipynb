{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "from cleverhans.tf2.attacks import fast_gradient_method as FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1 = pd.read_csv(\"https://raw.githubusercontent.com/Tdjaaleb/IDS_AdversarialML/main/Data/multi_data_train_0.csv\")\n",
    "df_train_2 = pd.read_csv(\"https://raw.githubusercontent.com/Tdjaaleb/IDS_AdversarialML/main/Data/multi_data_train_1.csv\")\n",
    "\n",
    "df_train = pd.concat([df_train_1, df_train_2], axis=0, ignore_index=True)\n",
    "\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/Tdjaaleb/IDS_AdversarialML/main/Data/multi_data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sort_index(axis=1)\n",
    "df_test = df_test.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensor_input_Train=tf.convert_to_tensor(df_train.drop(['intrusion','Dos','normal','Probe','R2L','U2R','label'], axis=1))\n",
    "df_tensor_output_Train=tf.convert_to_tensor(df_train[['Dos','normal','Probe','R2L','U2R']]) \n",
    "\n",
    "df_tensor_input_Test=tf.convert_to_tensor(df_test.drop(['intrusion','Dos','normal','Probe','R2L','U2R','label'], axis=1))\n",
    "df_tensor_output_Test=tf.convert_to_tensor(df_test[['Dos','normal','Probe','R2L','U2R']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = tf.keras.Sequential()\n",
    "mlp.add(tf.keras.layers.Dense(units=50, input_dim=df_tensor_input_Train.shape[1], activation=\"sigmoid\"))\n",
    "mlp.add(tf.keras.layers.Dense(units=5, activation=\"sigmoid\"))\n",
    "mlp.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "mlp.summary()\n",
    "\n",
    "mlp.fit(df_tensor_input_Train, df_tensor_output_Train, epochs=100, batch_size=512)\n",
    "\n",
    "mlp.evaluate(df_tensor_input_Test, df_tensor_output_Test)\n",
    "\n",
    "pred = mlp.predict(df_tensor_input_Test)\n",
    "\n",
    "pred_df = pd.DataFrame(pred,columns=df_train[['Dos','normal','Probe','R2L','U2R']].columns)\n",
    "\n",
    "pred_df = pred_df.idxmax(axis=1)\n",
    "\n",
    "print(\"Recall Score - \",recall_score(df_test['label'],pred_df, average='micro'))\n",
    "print(\"F1 Score - \",f1_score(df_test['label'],pred_df,average='micro'))\n",
    "print(\"Precision Score - \",precision_score(df_test['label'],pred_df,average='micro'))\n",
    "print(\"Accuracy Score - \", accuracy_score(df_test['label'],pred_df))\n",
    "\n",
    "cm = confusion_matrix(df_test['label'], pred_df)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "\n",
    "clf = classification_report(df_test['label'],pred_df)\n",
    "print(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = FGSM.fast_gradient_method(mlp, df_tensor_input_Train, 0.2, np.inf)\n",
    "adv_df = pd.DataFrame(adv.numpy())\n",
    "adv_df.columns = df_train.columns.drop(['intrusion','Dos','normal','Probe','R2L','U2R','label'])\n",
    "\n",
    "mlp.fit(adv_df, df_tensor_output_Train, epochs=10)\n",
    "\n",
    "pred = mlp.predict(df_tensor_input_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred,columns=df_train[['Dos','normal','Probe','R2L','U2R']].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.max(axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.axes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.iloc[[1]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[pred_df.axes[1][2]].iloc[[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.iloc[[0]].max(axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_state=[]\n",
    "\n",
    "for i in range(0,pred_df.shape[0]):\n",
    "  for j in range(0,pred_df.shape[1]):\n",
    "    if pred_df[pred_df.axes[1][j]].iloc[[i]][0]==pred_df.iloc[[i]].max(axis=1)[0]:\n",
    "        pred_state.append(pred_df.axes[1][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in range(0,pred.shape[1]):\n",
    "  for i in range(0,pred.shape[0]):\n",
    "    pred[i][j] = int(round(pred[i][j]))\n",
    "\n",
    "pred_df = pd.DataFrame(pred,columns=Train[['Dos','normal','Probe','R2L','U2R']].columns)\n",
    "\n",
    "print(\"Recall Score - \",recall_score(Test[['Dos','normal','Probe','R2L','U2R']],pred_df.astype('uint8'),average='micro'))\n",
    "print(\"F1 Score - \",f1_score(Test[['Dos','normal','Probe','R2L','U2R']],pred_df.astype('uint8'),average='micro'))\n",
    "print(\"Precision Score - \",precision_score(Test[['Dos','normal','Probe','R2L','U2R']],pred_df.astype('uint8'),average='micro'))\n",
    "print(\"Accuracy Score - \", accuracy_score(Test[['Dos','normal','Probe','R2L','U2R']],pred_df.astype('uint8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_state = []\n",
    "pred_state = []\n",
    "\n",
    "for i in range(0,pred.shape[0]):\n",
    "    lab = [\"Dos\",\"normal\",\"Probe\",\"R2L\",\"U2R\"]\n",
    "    for j in range(0,pred.shape[1]):\n",
    "        if pred[i][j]==1:\n",
    "            true_state.append(Test[\"label\"])\n",
    "            pred_state.append(lab[j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "585742c609c345d4844d7994d9eb5b28b097438423bf8e3de0d5b56aef578af1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
